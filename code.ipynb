{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.data_module.udpos_dataset import UDPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpos = UDPOS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Dataset Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of training examples in each dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(udpos.train)}\")\n",
    "print(f\"Number of validation examples: {len(udpos.val)}\")\n",
    "print(f\"Number of testing examples: {len(udpos.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what each example looks like. Here,\n",
    "* The `text` field will be used as feature for the POS tagger model.\n",
    "* The `udtags` field will be used as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " al - zaman : american forces killed shaikh abdullah al - ani , the preacher at the mosque in the town of qaim , near the syrian border .\n",
      "UD Tags\n",
      " ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "example = vars(udpos.train.examples[0])\n",
    "print(\"Text\\n\", ' '.join(example['text']))\n",
    "print(\"UD Tags\\n\", example['udtags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2. Analyzing Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields are preprocessed. This means, the tokenization step has been performed. Let's check what the vocabulary size is for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8866\n",
      "Unique tokens in UD_TAG vocabulary: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(udpos.TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in UD_TAG vocabulary: {len(udpos.UD_TAGS.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the top 10 most common tokens in `text` are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common tokens in text are as follows:\n",
      " 1) the   has count  9076\n",
      " 2) .     has count  8640\n",
      " 3) ,     has count  7021\n",
      " 4) to    has count  5137\n",
      " 5) and   has count  5002\n",
      " 6) a     has count  3782\n",
      " 7) of    has count  3622\n",
      " 8) i     has count  3379\n",
      " 9) in    has count  3112\n",
      "10) is    has count  2239\n"
     ]
    }
   ],
   "source": [
    "TOP = 10\n",
    "\n",
    "print(f\"Top {TOP} most common tokens in text are as follows:\")\n",
    "\n",
    "for i, (token, count) in enumerate(udpos.TEXT.vocab.freqs.most_common(TOP)):\n",
    "    print(f\"{i+1:>2}) {token:<5} has count {count:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how many distinct POS tags there are in `udtags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 in total which are as follows\n",
      " 1)   NOUN (34781)\n",
      " 2)  PUNCT (23679)\n",
      " 3)   VERB (23081)\n",
      " 4)   PRON (18577)\n",
      " 5)    ADP (17638)\n",
      " 6)    DET (16285)\n",
      " 7)  PROPN (12946)\n",
      " 8)    ADJ (12477)\n",
      " 9)    AUX (12343)\n",
      "10)    ADV (10548)\n",
      "11)  CCONJ (6707)\n",
      "12)   PART (5567)\n",
      "13)    NUM (3999)\n",
      "14)  SCONJ (3843)\n",
      "15)      X (847)\n",
      "16)   INTJ (688)\n",
      "17)    SYM (599)\n"
     ]
    }
   ],
   "source": [
    "unique_pos_tags = len(udpos.UD_TAGS.vocab)\n",
    "\n",
    "print(f\"There are {unique_pos_tags} in total which are as follows\")\n",
    "\n",
    "for i, (tag, count) in enumerate(udpos.UD_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{i+1:>2}) {tag:>6} ({count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Batch Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use vectorzied computation to be efficient. For that, we need to iterate over the dataset in batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the iterator looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 64]) torch.Size([44, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.train_dataloader():\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64]) torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.val_dataloader():\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch has two objects containing `text` and `udtags`. The shape of both the fields is $x \\times 64$. Here's what this means:\n",
    "\n",
    "* **Number of steps**: x. Meaning, there are x sequential tokens. \n",
    "* **Batches**: 64. Meaning, there are 64 different sequences.\n",
    "\n",
    "Let's look at a breakdown of one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: who ( thus she fear same ironically the <unk> hope rent best jen 1 as because nice i harry the we the they my enrononline perfect teresa ‘’ the 46 03/21/2001 ( i maybe do this he change i the hi ok <unk> my it what from it 30 also are if i 01/26/2001 i if as i our if after debbie and these\n",
      "Tags: PRON PUNCT ADV PRON NOUN ADJ ADV DET NUM VERB VERB ADJ PROPN X ADV SCONJ ADJ PRON PROPN DET PRON DET PRON PRON PROPN ADJ PROPN PUNCT DET NUM NUM PUNCT PRON ADV AUX PRON PRON VERB PRON DET INTJ INTJ NUM PRON PRON PRON ADP PRON NUM ADV AUX SCONJ PRON NUM PRON SCONJ ADP PRON PRON SCONJ ADP PROPN CCONJ DET\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.train_dataloader():\n",
    "    text, tags = batch\n",
    "\n",
    "    print(\"Text:\", \" \".join(udpos.TEXT.vocab.itos[i]\n",
    "                            for i in text[0]))\n",
    "    print(\"Tags:\", \" \".join(udpos.UD_TAGS.vocab.itos[i]\n",
    "                            for i in tags[0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing the filed values, their shape becomes $64 \\times 65$. So each row is now one example sentence. Thus the sentence makes sense when we print it. In the LSTM model, we'll iterate over timesteps and look at a batch of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first specify some hyperparameter values. These will dictate the architectural constraints of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Base LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(num_inputs=EMBEDDING_DIM, num_hiddens=HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "W_xf  with shape torch.Size([100, 128])\n",
      "W_hf  with shape torch.Size([128, 128])\n",
      "b_f   with shape torch.Size([128])\n",
      "W_xi  with shape torch.Size([100, 128])\n",
      "W_hi  with shape torch.Size([128, 128])\n",
      "b_i   with shape torch.Size([128])\n",
      "W_xo  with shape torch.Size([100, 128])\n",
      "W_ho  with shape torch.Size([128, 128])\n",
      "b_o   with shape torch.Size([128])\n",
      "W_xc  with shape torch.Size([100, 128])\n",
      "W_hc  with shape torch.Size([128, 128])\n",
      "b_c   with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<5} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = torch.randn((65, BATCH_SIZE, EMBEDDING_DIM))\n",
    "outputs, (H, C) = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 65\n",
      "Shape of Hidden State: torch.Size([64, 128])\n",
      "Shape of Memory Cell State: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of outputs:\", len(outputs))\n",
    "print(\"Shape of Hidden State:\", H.shape)\n",
    "print(\"Shape of Memory Cell State:\", C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dimensions match expected values. So our implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.bi_lstm import BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(num_inputs=EMBEDDING_DIM, num_hiddens=HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "forward_lstm.W_xf    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_hf    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_f     with shape torch.Size([128])\n",
      "forward_lstm.W_xi    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_hi    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_i     with shape torch.Size([128])\n",
      "forward_lstm.W_xo    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_ho    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_o     with shape torch.Size([128])\n",
      "forward_lstm.W_xc    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_hc    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_c     with shape torch.Size([128])\n",
      "backward_lstm.W_xf   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_hf   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_f    with shape torch.Size([128])\n",
      "backward_lstm.W_xi   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_hi   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_i    with shape torch.Size([128])\n",
      "backward_lstm.W_xo   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_ho   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_o    with shape torch.Size([128])\n",
      "backward_lstm.W_xc   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_hc   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_c    with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<20} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (f_h, b_h) = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape: 65\n",
      "Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Shape of backward Hidden State: torch.Size([64, 128])\n",
      "Shape of backward Memory Cell: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Outputs shape:\", len(outputs))\n",
    "print(\"Shape of Forward Hidden State:\", f_h[0].shape)\n",
    "print(\"Shape of Forward Memory Cell:\", f_h[1].shape)\n",
    "print(\"Shape of backward Hidden State:\", b_h[0].shape)\n",
    "print(\"Shape of backward Memory Cell:\", b_h[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output, hidden state, and memory cell shapes are as expected. So the implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Deep LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.deep_lstm import DeepLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLSTM(num_inputs=EMBEDDING_DIM,\n",
    "                 num_hiddens=HIDDEN_DIM,\n",
    "                 num_layers=NUM_LAYERS,\n",
    "                 bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "layers.0.W_xf        with shape torch.Size([100, 128])\n",
      "layers.0.W_hf        with shape torch.Size([128, 128])\n",
      "layers.0.b_f         with shape torch.Size([128])\n",
      "layers.0.W_xi        with shape torch.Size([100, 128])\n",
      "layers.0.W_hi        with shape torch.Size([128, 128])\n",
      "layers.0.b_i         with shape torch.Size([128])\n",
      "layers.0.W_xo        with shape torch.Size([100, 128])\n",
      "layers.0.W_ho        with shape torch.Size([128, 128])\n",
      "layers.0.b_o         with shape torch.Size([128])\n",
      "layers.0.W_xc        with shape torch.Size([100, 128])\n",
      "layers.0.W_hc        with shape torch.Size([128, 128])\n",
      "layers.0.b_c         with shape torch.Size([128])\n",
      "layers.1.W_xf        with shape torch.Size([128, 128])\n",
      "layers.1.W_hf        with shape torch.Size([128, 128])\n",
      "layers.1.b_f         with shape torch.Size([128])\n",
      "layers.1.W_xi        with shape torch.Size([128, 128])\n",
      "layers.1.W_hi        with shape torch.Size([128, 128])\n",
      "layers.1.b_i         with shape torch.Size([128])\n",
      "layers.1.W_xo        with shape torch.Size([128, 128])\n",
      "layers.1.W_ho        with shape torch.Size([128, 128])\n",
      "layers.1.b_o         with shape torch.Size([128])\n",
      "layers.1.W_xc        with shape torch.Size([128, 128])\n",
      "layers.1.W_hc        with shape torch.Size([128, 128])\n",
      "layers.1.b_c         with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<20} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, Hs = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([65, 64, 128])\n",
      "Layer 1: Shape of Hidden State: torch.Size([64, 128])\n",
      "Layer 1: Shape of Memory Cell: torch.Size([64, 128])\n",
      "Layer 2: Shape of Hidden State: torch.Size([64, 128])\n",
      "Layer 2: Shape of Memory Cell: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "for i, h in enumerate(Hs):\n",
    "    print(f\"Layer {i+1}: Shape of Hidden State:\", h[0].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Memory Cell:\", h[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Deep Bidrectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLSTM(num_inputs=EMBEDDING_DIM,\n",
    "                 num_hiddens=HIDDEN_DIM,\n",
    "                 num_layers=NUM_LAYERS,\n",
    "                 bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "layers.0.forward_lstm.W_xf with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_f with shape torch.Size([128])\n",
      "layers.0.forward_lstm.W_xi with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_i with shape torch.Size([128])\n",
      "layers.0.forward_lstm.W_xo with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_o with shape torch.Size([128])\n",
      "layers.0.forward_lstm.W_xc with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_c with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xf with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_f with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xi with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_i with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xo with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_o with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xc with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_c with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xf with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_f with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xi with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_i with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xo with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_o with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xc with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_c with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xf with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_f with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xi with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_i with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xo with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_o with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xc with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_c with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<20} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, Hs = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([65, 64, 256])\n",
      "Layer 1: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 1: Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Layer 1: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 1: Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Memory Cell: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "for i, h in enumerate(Hs):\n",
    "    print(f\"Layer {i+1}: Shape of Forward Hidden State:\", h[0][0].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Forward Memory Cell:\", h[0][1].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Forward Hidden State:\", h[1][0].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Forward Memory Cell:\", h[1][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.pos_tagger import PosTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = PosTagger(num_inputs=len(udpos.TEXT.vocab),\n",
    "                  embedding_dim=100,\n",
    "                  num_hiddens=128,\n",
    "                  num_outputs=len(udpos.UD_TAGS.vocab),\n",
    "                  bidirectional=True,\n",
    "                  num_layers=2,\n",
    "                  padding_idx=udpos.TEXT.vocab[udpos.TEXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features.shape: torch.Size([50, 64])\n",
      "Labels.shape: torch.Size([50, 64])\n",
      "Predictions.shape: torch.Size([50, 64, 18])\n",
      "Loss: 2.8818650245666504\n",
      "Accuracy: 0.0032959789969027042\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.train_dataloader():\n",
    "    X, y = batch\n",
    "    y_hat = model(X)\n",
    "    \n",
    "    print(f\"Features.shape: {X.shape}\")\n",
    "    print(f\"Labels.shape: {y.shape}\")\n",
    "    print(f\"Predictions.shape: {y_hat.shape}\")\n",
    "    \n",
    "    y_hat = y_hat.reshape(-1, y_hat.shape[-1])\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "    loss = model.loss(y_hat, y)\n",
    "    acc = model.accuracy(y_hat, y)\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    print(f\"Accuracy: {acc.item()}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
