{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.data_module.udpos_dataset import UDPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpos = UDPOS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Dataset Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of training examples in each dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(udpos.train)}\")\n",
    "print(f\"Number of validation examples: {len(udpos.val)}\")\n",
    "print(f\"Number of testing examples: {len(udpos.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what each example looks like. Here,\n",
    "* The `text` field will be used as feature for the POS tagger model.\n",
    "* The `udtags` field will be used as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      " al - zaman : american forces killed shaikh abdullah al - ani , the preacher at the mosque in the town of qaim , near the syrian border .\n",
      "UD Tags\n",
      " ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "example = vars(udpos.train.examples[0])\n",
    "print(\"Text\\n\", ' '.join(example['text']))\n",
    "print(\"UD Tags\\n\", example['udtags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2. Analyzing Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields are preprocessed. This means, the tokenization step has been performed. Let's check what the vocabulary size is for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8866\n",
      "Unique tokens in UD_TAG vocabulary: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(udpos.TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in UD_TAG vocabulary: {len(udpos.UD_TAGS.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the top 10 most common tokens in `text` are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common tokens in text are as follows:\n",
      " 1) the   has count  9076\n",
      " 2) .     has count  8640\n",
      " 3) ,     has count  7021\n",
      " 4) to    has count  5137\n",
      " 5) and   has count  5002\n",
      " 6) a     has count  3782\n",
      " 7) of    has count  3622\n",
      " 8) i     has count  3379\n",
      " 9) in    has count  3112\n",
      "10) is    has count  2239\n"
     ]
    }
   ],
   "source": [
    "TOP = 10\n",
    "\n",
    "print(f\"Top {TOP} most common tokens in text are as follows:\")\n",
    "\n",
    "for i, (token, count) in enumerate(udpos.TEXT.vocab.freqs.most_common(TOP)):\n",
    "    print(f\"{i+1:>2}) {token:<5} has count {count:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how many distinct POS tags there are in `udtags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 in total which are as follows\n",
      " 1)   NOUN (34781)\n",
      " 2)  PUNCT (23679)\n",
      " 3)   VERB (23081)\n",
      " 4)   PRON (18577)\n",
      " 5)    ADP (17638)\n",
      " 6)    DET (16285)\n",
      " 7)  PROPN (12946)\n",
      " 8)    ADJ (12477)\n",
      " 9)    AUX (12343)\n",
      "10)    ADV (10548)\n",
      "11)  CCONJ (6707)\n",
      "12)   PART (5567)\n",
      "13)    NUM (3999)\n",
      "14)  SCONJ (3843)\n",
      "15)      X (847)\n",
      "16)   INTJ (688)\n",
      "17)    SYM (599)\n"
     ]
    }
   ],
   "source": [
    "unique_pos_tags = len(udpos.UD_TAGS.vocab)\n",
    "\n",
    "print(f\"There are {unique_pos_tags} in total which are as follows\")\n",
    "\n",
    "for i, (tag, count) in enumerate(udpos.UD_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{i+1:>2}) {tag:>6} ({count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Batch Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use vectorzied computation to be efficient. For that, we need to iterate over the dataset in batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the iterator looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 64]) torch.Size([62, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.train_dataloader():\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64]) torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.val_dataloader():\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each batch has two objects containing `text` and `udtags`. The shape of both the fields is $x \\times 64$. Here's what this means:\n",
    "\n",
    "* **Number of steps**: x. Meaning, there are x sequential tokens. \n",
    "* **Batches**: 64. Meaning, there are 64 different sequences.\n",
    "\n",
    "Let's look at a breakdown of one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: thank that they you will service i sorry when this vince have i it because lorie www are will 3 we the and <unk> tiffany i from the a an i i power there most sorry <unk> juan margaret aquarius the it game little sounds thank absolute we we we john that we like you mark the criminal if take he five \" one\n",
      "Tags: VERB DET PRON PRON AUX NOUN PRON INTJ ADV DET PROPN VERB PRON PRON ADP X PROPN AUX AUX X PRON DET CCONJ X PROPN PRON SCONJ DET DET DET PRON PRON NOUN PRON ADJ ADJ NOUN PROPN PROPN PROPN DET PRON NOUN ADV VERB VERB ADJ PRON PRON PRON PROPN DET PRON ADP PRON PROPN DET ADJ SCONJ VERB PRON NUM PUNCT NUM\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.train_dataloader():\n",
    "    text, tags = batch\n",
    "\n",
    "    print(\"Text:\", \" \".join(udpos.TEXT.vocab.itos[i]\n",
    "                            for i in text[0]))\n",
    "    print(\"Tags:\", \" \".join(udpos.UD_TAGS.vocab.itos[i]\n",
    "                            for i in tags[0]))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing the filed values, their shape becomes $64 \\times 65$. So each row is now one example sentence. Thus the sentence makes sense when we print it. In the LSTM model, we'll iterate over timesteps and look at a batch of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first specify some hyperparameter values. These will dictate the architectural constraints of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Base LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(num_inputs=EMBEDDING_DIM, num_hiddens=HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "W_xf  with shape torch.Size([100, 128])\n",
      "W_hf  with shape torch.Size([128, 128])\n",
      "b_f   with shape torch.Size([128])\n",
      "W_xi  with shape torch.Size([100, 128])\n",
      "W_hi  with shape torch.Size([128, 128])\n",
      "b_i   with shape torch.Size([128])\n",
      "W_xo  with shape torch.Size([100, 128])\n",
      "W_ho  with shape torch.Size([128, 128])\n",
      "b_o   with shape torch.Size([128])\n",
      "W_xc  with shape torch.Size([100, 128])\n",
      "W_hc  with shape torch.Size([128, 128])\n",
      "b_c   with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<5} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = torch.randn((65, BATCH_SIZE, EMBEDDING_DIM))\n",
    "outputs, (H, C) = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 65\n",
      "Shape of Hidden State: torch.Size([64, 128])\n",
      "Shape of Memory Cell State: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of outputs:\", len(outputs))\n",
    "print(\"Shape of Hidden State:\", H.shape)\n",
    "print(\"Shape of Memory Cell State:\", C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dimensions match expected values. So our implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.bi_lstm import BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(num_inputs=EMBEDDING_DIM, num_hiddens=HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "forward_lstm.W_xf    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_hf    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_f     with shape torch.Size([128])\n",
      "forward_lstm.W_xi    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_hi    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_i     with shape torch.Size([128])\n",
      "forward_lstm.W_xo    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_ho    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_o     with shape torch.Size([128])\n",
      "forward_lstm.W_xc    with shape torch.Size([100, 128])\n",
      "forward_lstm.W_hc    with shape torch.Size([128, 128])\n",
      "forward_lstm.b_c     with shape torch.Size([128])\n",
      "backward_lstm.W_xf   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_hf   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_f    with shape torch.Size([128])\n",
      "backward_lstm.W_xi   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_hi   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_i    with shape torch.Size([128])\n",
      "backward_lstm.W_xo   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_ho   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_o    with shape torch.Size([128])\n",
      "backward_lstm.W_xc   with shape torch.Size([100, 128])\n",
      "backward_lstm.W_hc   with shape torch.Size([128, 128])\n",
      "backward_lstm.b_c    with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<20} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (f_h, b_h) = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape: 65\n",
      "Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Shape of backward Hidden State: torch.Size([64, 128])\n",
      "Shape of backward Memory Cell: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Outputs shape:\", len(outputs))\n",
    "print(\"Shape of Forward Hidden State:\", f_h[0].shape)\n",
    "print(\"Shape of Forward Memory Cell:\", f_h[1].shape)\n",
    "print(\"Shape of backward Hidden State:\", b_h[0].shape)\n",
    "print(\"Shape of backward Memory Cell:\", b_h[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output, hidden state, and memory cell shapes are as expected. So the implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Deep LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.deep_lstm import DeepLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLSTM(num_inputs=EMBEDDING_DIM,\n",
    "                 num_hiddens=HIDDEN_DIM,\n",
    "                 num_layers=NUM_LAYERS,\n",
    "                 bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "layers.0.W_xf        with shape torch.Size([100, 128])\n",
      "layers.0.W_hf        with shape torch.Size([128, 128])\n",
      "layers.0.b_f         with shape torch.Size([128])\n",
      "layers.0.W_xi        with shape torch.Size([100, 128])\n",
      "layers.0.W_hi        with shape torch.Size([128, 128])\n",
      "layers.0.b_i         with shape torch.Size([128])\n",
      "layers.0.W_xo        with shape torch.Size([100, 128])\n",
      "layers.0.W_ho        with shape torch.Size([128, 128])\n",
      "layers.0.b_o         with shape torch.Size([128])\n",
      "layers.0.W_xc        with shape torch.Size([100, 128])\n",
      "layers.0.W_hc        with shape torch.Size([128, 128])\n",
      "layers.0.b_c         with shape torch.Size([128])\n",
      "layers.1.W_xf        with shape torch.Size([128, 128])\n",
      "layers.1.W_hf        with shape torch.Size([128, 128])\n",
      "layers.1.b_f         with shape torch.Size([128])\n",
      "layers.1.W_xi        with shape torch.Size([128, 128])\n",
      "layers.1.W_hi        with shape torch.Size([128, 128])\n",
      "layers.1.b_i         with shape torch.Size([128])\n",
      "layers.1.W_xo        with shape torch.Size([128, 128])\n",
      "layers.1.W_ho        with shape torch.Size([128, 128])\n",
      "layers.1.b_o         with shape torch.Size([128])\n",
      "layers.1.W_xc        with shape torch.Size([128, 128])\n",
      "layers.1.W_hc        with shape torch.Size([128, 128])\n",
      "layers.1.b_c         with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<20} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, Hs = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([65, 64, 128])\n",
      "Layer 1: Shape of Hidden State: torch.Size([64, 128])\n",
      "Layer 1: Shape of Memory Cell: torch.Size([64, 128])\n",
      "Layer 2: Shape of Hidden State: torch.Size([64, 128])\n",
      "Layer 2: Shape of Memory Cell: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "for i, h in enumerate(Hs):\n",
    "    print(f\"Layer {i+1}: Shape of Hidden State:\", h[0].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Memory Cell:\", h[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Deep Bidrectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLSTM(num_inputs=EMBEDDING_DIM,\n",
    "                 num_hiddens=HIDDEN_DIM,\n",
    "                 num_layers=NUM_LAYERS,\n",
    "                 bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of parameters making up the LSTM model\n",
      "layers.0.forward_lstm.W_xf with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_f with shape torch.Size([128])\n",
      "layers.0.forward_lstm.W_xi with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_i with shape torch.Size([128])\n",
      "layers.0.forward_lstm.W_xo with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_o with shape torch.Size([128])\n",
      "layers.0.forward_lstm.W_xc with shape torch.Size([100, 128])\n",
      "layers.0.forward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.0.forward_lstm.b_c with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xf with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_f with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xi with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_i with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xo with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_o with shape torch.Size([128])\n",
      "layers.0.backward_lstm.W_xc with shape torch.Size([100, 128])\n",
      "layers.0.backward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.0.backward_lstm.b_c with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xf with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_f with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xi with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_i with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xo with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_o with shape torch.Size([128])\n",
      "layers.1.forward_lstm.W_xc with shape torch.Size([256, 128])\n",
      "layers.1.forward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.1.forward_lstm.b_c with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xf with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_hf with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_f with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xi with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_hi with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_i with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xo with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_ho with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_o with shape torch.Size([128])\n",
      "layers.1.backward_lstm.W_xc with shape torch.Size([256, 128])\n",
      "layers.1.backward_lstm.W_hc with shape torch.Size([128, 128])\n",
      "layers.1.backward_lstm.b_c with shape torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"List of parameters making up the LSTM model\")\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(f\"{name:<20} with shape {params.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, Hs = model(dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([65, 64, 256])\n",
      "Layer 1: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 1: Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Layer 1: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 1: Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Memory Cell: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Hidden State: torch.Size([64, 128])\n",
      "Layer 2: Shape of Forward Memory Cell: torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", outputs.shape)\n",
    "\n",
    "for i, h in enumerate(Hs):\n",
    "    print(f\"Layer {i+1}: Shape of Forward Hidden State:\", h[0][0].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Forward Memory Cell:\", h[0][1].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Forward Hidden State:\", h[1][0].shape)\n",
    "    print(f\"Layer {i+1}: Shape of Forward Memory Cell:\", h[1][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.module.pos_tagger import PosTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = PosTagger(num_inputs=len(udpos.TEXT.vocab),\n",
    "                  embedding_dim=100,\n",
    "                  num_hiddens=128,\n",
    "                  num_outputs=len(udpos.UD_TAGS.vocab),\n",
    "                  bidirectional=True,\n",
    "                  num_layers=2,\n",
    "                  padding_idx=udpos.TEXT.vocab[udpos.TEXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features.shape: torch.Size([70, 64])\n",
      "Labels.shape: torch.Size([70, 64])\n",
      "Predictions.shape: torch.Size([70, 64, 18])\n",
      "Loss: 2.8674864768981934\n",
      "Accuracy: 0.00977426115423441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "for batch in udpos.train_dataloader():\n",
    "    X, y = batch\n",
    "    y_hat = model(X)\n",
    "    \n",
    "    print(f\"Features.shape: {X.shape}\")\n",
    "    print(f\"Labels.shape: {y.shape}\")\n",
    "    print(f\"Predictions.shape: {y_hat.shape}\")\n",
    "    \n",
    "    y_hat = y_hat.reshape(-1, y_hat.shape[-1])\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "    loss = model.loss(y_hat, y)\n",
    "    acc = model.accuracy(y_hat, y)\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    print(f\"Accuracy: {acc.item()}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training mechanism is basically two steps per epoch.\n",
    "\n",
    "1. **Training step**: Go through the entire training dataset in batches. Feed and train the model using said batches.\n",
    "2. **Validation step**: Check how the model performs after each training step. Don't train the model on this dataset. This is just to judge how well the model does in unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.loops import train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "udpos = UDPOS()\n",
    "model = PosTagger(num_inputs=len(udpos.TEXT.vocab),\n",
    "                  embedding_dim=100,\n",
    "                  num_hiddens=64,\n",
    "                  num_outputs=len(udpos.UD_TAGS.vocab),\n",
    "                  bidirectional=True,\n",
    "                  num_layers=1,\n",
    "                  padding_idx=udpos.TEXT.vocab[udpos.TEXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 in (36.30) secs\n",
      "\tTrain Loss: 0.771 | Train Acc: 84.51%\n",
      "\t Val. Loss: 1.260 |  Val. Acc: 66.52%\n",
      "Epoch: 02 in (35.75) secs\n",
      "\tTrain Loss: 0.228 | Train Acc: 93.31%\n",
      "\t Val. Loss: 0.827 |  Val. Acc: 77.41%\n",
      "Epoch: 03 in (35.72) secs\n",
      "\tTrain Loss: 0.149 | Train Acc: 95.54%\n",
      "\t Val. Loss: 0.642 |  Val. Acc: 80.64%\n",
      "Epoch: 04 in (35.56) secs\n",
      "\tTrain Loss: 0.106 | Train Acc: 96.80%\n",
      "\t Val. Loss: 0.544 |  Val. Acc: 83.42%\n",
      "Epoch: 05 in (35.40) secs\n",
      "\tTrain Loss: 0.082 | Train Acc: 97.52%\n",
      "\t Val. Loss: 0.480 |  Val. Acc: 85.25%\n",
      "Epoch: 06 in (35.20) secs\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.98%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 86.41%\n",
      "Epoch: 07 in (35.14) secs\n",
      "\tTrain Loss: 0.057 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.399 |  Val. Acc: 87.71%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10216\\896275724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mudpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Projects\\POS_Tagging\\src\\utils\\loops.py\u001b[0m in \u001b[0;36mtrain_epochs\u001b[1;34m(model, data, epochs)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\POS_Tagging\\src\\utils\\loops.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m# Back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Projects\\POS_Tagging\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, accuracies = train_epochs(model, udpos, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
